# -*- coding: utf-8 -*-
"""IR_Project_Assignment_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tBp9VTTkYIuKU-Rr8QaI2i4vaZ-0E-sw
"""

import pandas as pd
import re
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from rank_bm25 import BM25Okapi

# -----------------------------
# 1) LOAD DATA
# -----------------------------
df = pd.read_csv("Articles.csv", encoding="ISO-8859-1")
df["Article"] = df["Article"].fillna("")

# 2) CLEANING
# -----------------------------
def clean(text):
    text = text.lower()
    text = re.sub(r"[^\w\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

df["clean"] = df["Article"].apply(clean)
docs = df["clean"].tolist()
print("Total documents:", len(docs))

# 3) TF-IDF VECTORIZATION
# -----------------------------
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(docs)
print("TF-IDF shape:", tfidf_matrix.shape)

# 4) BM25 SETUP
# -----------------------------
def tokenize(text):
    return text.split()

tokenized_docs = [tokenize(t) for t in docs]
bm25 = BM25Okapi(tokenized_docs)

def bm25_search(query, k=10):
    q = tokenize(clean(query))
    scores = bm25.get_scores(q)
    ranked = np.argsort(-scores)[:k]
    return ranked.tolist()
# Test
print("BM25:", bm25_search("bengaluru"))

# 5) HYBRID SEARCH
# -----------------------------
def hybrid_search(query, alpha=0.5, k=10):
    # Clean query
    q_clean = clean(query)

    # TF-IDF scores
    q_vec = vectorizer.transform([q_clean])
    tfidf_scores = cosine_similarity(q_vec, tfidf_matrix).flatten()

    # BM25 scores
    q_tokens = tokenize(q_clean)
    bm25_scores = np.array(bm25.get_scores(q_tokens))

    # Avoid division by zero
    if tfidf_scores.max() == 0:
        tfidf_norm = tfidf_scores
    else:
        tfidf_norm = tfidf_scores / tfidf_scores.max()

    if bm25_scores.max() == 0:
        bm25_norm = bm25_scores
    else:
        bm25_norm = bm25_scores / bm25_scores.max()

    # Combine
    hybrid = alpha * bm25_norm + (1 - alpha) * tfidf_norm

    ranked = np.argsort(-hybrid)[:k]
    return ranked.tolist()

# Test
print("Hybrid:", hybrid_search("bengaluru", alpha=0.6))

# ------------------------------
# Step: Find real Python indexes for relevant documents
# ------------------------------
query = "bengaluru"
print("Documents containing the query:")

for i, doc in enumerate(docs):
    if query.lower() in doc.lower():
        print(i, doc[:100])  # first 100 characters as snippet

import time

# -----------------------------
# 6) EVALUATION METRICS
# -----------------------------
def precision_at_k(ranked, relevant, k=5):
    return len([d for d in ranked[:k] if d in relevant]) / k

def recall_at_k(ranked, relevant, k=5):
    return len([d for d in ranked[:k] if d in relevant]) / len(relevant) if relevant else 0.0

def f1_at_k(ranked, relevant, k=5):
    p = precision_at_k(ranked, relevant, k)
    r = recall_at_k(ranked, relevant, k)
    if p + r == 0:
        return 0.0
    return 2 * (p * r) / (p + r)

def average_precision(ranked, relevant):
    if not relevant:
        return 0.0
    hits = 0
    score = 0
    for i, d in enumerate(ranked, 1):
        if d in relevant:
            hits += 1
            score += hits / i
    return score / len(relevant)

query = "bengaluru"
relevant_docs = {616, 1483, 1484, 1497, 1745}

# Measure time for hybrid search
start_time = time.time()
ranked_docs = hybrid_search(query, alpha=0.6, k=10)
end_time = time.time()

print("Query time (seconds):", round(end_time - start_time, 5))

# Compute evaluation metrics
P5 = precision_at_k(ranked_docs, relevant_docs, k=5)
R5 = recall_at_k(ranked_docs, relevant_docs, k=5)
F1 = f1_at_k(ranked_docs, relevant_docs, k=5)
AP = average_precision(ranked_docs, relevant_docs)

print("P@5:", P5)
print("Recall@5:", R5)
print("F1@5:", F1)
print("AP:", AP)

